{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x8cH2rRlfU7i"
      },
      "source": [
        "#### CSCE 670 :: Information Storage & Retrieval :: Texas A&M University :: Spring 2023\n",
        "\n",
        "\n",
        "# Project 2: Implicit Recommender Systems"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sgS43jLbL77U"
      },
      "source": [
        "# Part 1. Recommendations with Implicit Feedback \n",
        "\n",
        "For this part, we're going to build a simple **non-personalized** implicit recommendation algorithm. Since feedback like user clicks, purchases, and views is much more widespread than explicit ratings, implicit recommenders offer great opportunities for far-reaching impact. \n",
        "\n",
        "Concretely, the task of implicit recommendation is to recommend items to users based on implicit signals from users, i.e., we only know what items a user is interested in, but have no idea what items the user dislikes. So for this case, the dataset we could use for this implicit recommendation experiment only contains binary data with 1 representing that the user likes the item, and with 0 representing that we don't know the user's preference towards the item. Because of this, we cannot use the same evaluation method as explicit recommendation. Instead, we need to evaluate the implicit recommendation quality by a ranking task.\n",
        "\n",
        "For this part, we will:\n",
        "* load and process the MovieLens 1M dataset,\n",
        "* transfer the explicit dataset to implicit one,\n",
        "* build a non-personalized implicit recommender, \n",
        "* and evaluate your recommender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e5ak01BL77T"
      },
      "source": [
        "To start out, we need to prepare the data. We will use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this homework. Lucky for you, we are providing the file containing the ratings -- ratings.dat  -- so all you need to do is load the ratings.dat file in the notebook as a DataFrame variable using the Pandas library. The code to do this has been provided in the next cell, but you need to run it. The resulting data variables are: train_mat is the numpy array variable for training data of size (#users, #items) with non-zero entries representing user-item ratings, and zero entries representing unknown user-item ratings; and test_mat is the numpy array variable for testing data of size (#users, #items)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaocWZ94MD-e",
        "outputId": "fd13b746-c353-4824-de93-9dd501ec7ed2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3706,)\n",
            "(6040,)\n",
            "         UserID  MovieID  Rating  Timestamp  movieID\n",
            "0             0        0       5  978300760        0\n",
            "2             0        2       3  978301968        2\n",
            "3             0        3       4  978300275        3\n",
            "4             0        4       5  978824291        4\n",
            "5             0        5       3  978302268        5\n",
            "...         ...      ...     ...        ...      ...\n",
            "1000202    6039      336       4  956704996      336\n",
            "1000204    6039      772       1  956716541      772\n",
            "1000205    6039     1106       5  956704887     1106\n",
            "1000207    6039      152       4  956715648      152\n",
            "1000208    6039       26       4  956715569       26\n",
            "\n",
            "[699628 rows x 5 columns]\n",
            "         UserID  MovieID  Rating  Timestamp  movieID\n",
            "1             0        1       3  978302109        1\n",
            "7             0        7       5  978300719        7\n",
            "8             0        8       4  978302268        8\n",
            "10            0       10       5  978824268       10\n",
            "13            0       13       4  978302124       13\n",
            "...         ...      ...     ...        ...      ...\n",
            "1000197    6039     1098       3  956715288     1098\n",
            "1000199    6039     2633       5  956716207     2633\n",
            "1000201    6039      756       4  957717322      756\n",
            "1000203    6039      147       3  956715518      147\n",
            "1000206    6039      365       5  956704746      365\n",
            "\n",
            "[300581 rows x 5 columns]\n",
            "(699628, 5)\n",
            "(300581, 5)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "data_df = pd.read_csv('./ratings.dat', sep='::', names=[\"UserID\", \"MovieID\", \"Rating\", \"Timestamp\"], engine='python')\n",
        "\n",
        "# First, generate dictionaries for mapping old id to new id for users and movies\n",
        "unique_MovieID = data_df['MovieID'].unique()\n",
        "unique_UserID = data_df['UserID'].unique()\n",
        "print(unique_MovieID.shape)\n",
        "print(unique_UserID.shape)\n",
        "j = 0\n",
        "user_old2new_id_dict = dict()\n",
        "for u in unique_UserID:\n",
        "    user_old2new_id_dict[u] = j\n",
        "    j += 1\n",
        "j = 0\n",
        "\n",
        "movie_old2new_id_dict = dict()\n",
        "for i in unique_MovieID:\n",
        "    movie_old2new_id_dict[i] = j\n",
        "    j += 1\n",
        "\n",
        "# Then, use the generated dictionaries to reindex UserID and MovieID in the data_df\n",
        "user_list = data_df['UserID'].values\n",
        "movie_list = data_df['MovieID'].values\n",
        "for j in range(len(data_df)):\n",
        "    user_list[j] = user_old2new_id_dict[user_list[j]]\n",
        "    movie_list[j] = movie_old2new_id_dict[movie_list[j]]\n",
        "data_df['UserID'] = user_list\n",
        "data_df['movieID'] = movie_list\n",
        "\n",
        "# generate train_df with 70% samples and test_df with 30% samples, and there should have no overlap between them.\n",
        "np.random.seed(0)\n",
        "train_index = np.random.random(len(data_df)) <= 0.7\n",
        "train_df = data_df[train_index]\n",
        "test_df = data_df[~train_index]\n",
        "print(train_df)\n",
        "print(test_df)\n",
        "print(train_df.shape)\n",
        "print(test_df.shape)\n",
        "\n",
        "# generate train_mat and test_mat\n",
        "num_user = len(data_df['UserID'].unique())\n",
        "num_movie = len(data_df['MovieID'].unique())\n",
        "\n",
        "train_mat = coo_matrix((train_df['Rating'].values, (train_df['UserID'].values, train_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()\n",
        "test_mat = coo_matrix((test_df['Rating'].values, (test_df['UserID'].values, test_df['MovieID'].values)), shape=(num_user, num_movie)).astype(float).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-sEqkSHL77V"
      },
      "source": [
        "It is very easy to transfer the explicit datasets you already generated to implicit ones: here, you will consider the watching behavior as the implicit signal showing that the user is interested in a movie. Thus, you can use the same train_df and test_df for implicit recommendation experiment with 'Rating' column ignored. And for train_mat and test_mat, you need to make all ratings to be value 1 and keep 0 entries the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av77NlOLL77V",
        "outputId": "8ed39b30-358e-4692-b68f-8c700509306a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "(6040, 3706)\n",
            "(6040, 3706)\n"
          ]
        }
      ],
      "source": [
        "# turn the explicit ratings to implicit feedback data\n",
        "train_mat = (train_mat > 0).astype(float)\n",
        "test_mat = (test_mat > 0).astype(float)\n",
        "print(train_mat)\n",
        "print(train_mat.shape)\n",
        "print(test_mat.shape)\n",
        "# print(test_mat)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mvun1WmYL77V"
      },
      "source": [
        "## Part 1a: Building the non-personalized recommender \n",
        "\n",
        "In this part, you need to build a non-personalized recommendation model to provide a ranked list of 50 movies as the recommendation for each user. The model is very simple: for each user, the recommendation list is to rank the unwatched movies by their **popularity**, where the popularity is the number of implicit feedback each movie gets. In this case, although it is non-personalized recommender, the recommendation results may be different for users because the unwatched movies are different across users.\n",
        "\n",
        "In the next cell, write your code to generate the ranked lists of movies by the popularity based recommendation algorithm for every user, store the result in a numpy array of size (#user, 50), entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. Print out the top-5 recommended movies and their popularity for the first user (with id 0).\n",
        "\n",
        "Hint: the popularity can only be calculated from train_mat, you cannot use test_mat here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9rcPRfhL77V",
        "outputId": "595171e5-1ede-4cee-d835-73914a9fb10a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3706,)\n",
            "[104. 124.  64. 113.  97.]\n",
            "\n",
            "Top 5 recommended movies for first user are:\n",
            "Movie ID: 104.0 is at 1 position with popularity of 2423\n",
            "Movie ID: 124.0 is at 2 position with popularity of 2086\n",
            "Movie ID: 64.0 is at 3 position with popularity of 2064\n",
            "Movie ID: 113.0 is at 4 position with popularity of 1862\n",
            "Movie ID: 97.0 is at 5 position with popularity of 1845\n"
          ]
        }
      ],
      "source": [
        "# Generate a ranked list of movies by the popularity based recommendation algorithm. And print out the id and popularity of the top5 movies for the first user.\n",
        "# Your Code Here...\n",
        "def non_personalized_recommender(train_mat):\n",
        "  popularity = train_mat.sum(axis=0).astype(int)\n",
        "  print(popularity.shape)\n",
        "  # print(np.sum(popularity))\n",
        "  top_50 = np.zeros((num_user,50))\n",
        "  \n",
        "\n",
        "  for u in range(num_user):\n",
        "    unwatched_movies = np.where(train_mat[u,:]== 0)[0]\n",
        "    # print('\\n',unwatched_movies)\n",
        "    temp = np.zeros((num_movie))\n",
        "    temp[unwatched_movies] = popularity[unwatched_movies]\n",
        "    # print(temp)\n",
        "    ranked_unwatched_movies = np.argsort(temp) \n",
        "    # print('\\n',ranked_unwatched_movies)\n",
        "    top_50_u = np.flip(ranked_unwatched_movies)[:50]\n",
        "    # print(top_50_u)\n",
        "    top_50[u,:] = top_50_u\n",
        "  return top_50,popularity\n",
        "\n",
        "top_50_list, popularity = non_personalized_recommender(train_mat)\n",
        "\n",
        "print(top_50_list[0,:5])\n",
        "\n",
        "\n",
        "print('\\nTop 5 recommended movies for first user are:')\n",
        "for i in range(5):\n",
        "  print('Movie ID:', top_50_list[0,i], 'is at', i+1, 'position with popularity of', popularity[int(top_50_list[0,i])])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WwzW8K6gL77V"
      },
      "source": [
        "## Part 1b: Evaluating the non-personalized recommender \n",
        "\n",
        "In this part, you need to evaluate your non-personalized recommendation by the held-out testing dataset test_mat for each user. For the implicit recommendation, two typical metrics are recall@k and precision@k. Here, you need to write the code to calculate recall@k and  precision@k for k=5, 20, 50 for each user, i.e., six metrics for every user. And please print out the average over all users for these six metrics.\n",
        "\n",
        "Hint: if a user does not have any testing samples in test_mat, do not include this user in the final averaged metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3I34QYzL77V",
        "outputId": "43bddd11-8b5b-4a51-8bbb-f8dcc8e300b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15 35 13 ...  6 41 98]\n",
            "Average Recall values for k = 5 is [0.03786131]\n",
            "Average Recall values for k = 20 is [0.10863614]\n",
            "Average Recall values for k = 50 is [0.19360902]\n",
            "Average Precision values for k = 5 is [0.27301325]\n",
            "Average Precision values for k = 20 is [0.21042219]\n",
            "Average Precision values for k = 50 is [0.15778477]\n"
          ]
        }
      ],
      "source": [
        "# Calculate recall@k and precision@k with k=5, 20, 50 and print out the average over all users for these 9 metrics.\n",
        "# Your Code Here...\n",
        "\n",
        "\n",
        "def recall_func(relevant_retrieved_docs,total_relevant):\n",
        "  return relevant_retrieved_docs/total_relevant\n",
        "\n",
        "def precision_func(relevant_retrieved_docs,k):\n",
        "  return relevant_retrieved_docs/k\n",
        "\n",
        "def calculate_parameters(top_50_list, test_mat):\n",
        "  recall5 = np.zeros((num_user,1))\n",
        "  precision5 = np.zeros((num_user,1))\n",
        "  recall20 = np.zeros((num_user,1))\n",
        "  precision20 = np.zeros((num_user,1))\n",
        "  recall50 = np.zeros((num_user,1))\n",
        "  precision50 = np.zeros((num_user,1))\n",
        "  # recall = np.zeros((num_user,3))\n",
        "  # precision = np.zeros((num_user,3))\n",
        "\n",
        "  total_relevant = test_mat.sum(axis=1).astype(int)\n",
        "  print(total_relevant)\n",
        "  invalid_user = np.where(total_relevant[:]== 0)[0]\n",
        "\n",
        "  for u in range(num_user):\n",
        "    relevant_retrieved_docs = 0\n",
        "    if u in invalid_user:\n",
        "      continue\n",
        "    #calculation for k = 5\n",
        "    for k in range(5):\n",
        "      relevant_retrieved_docs = relevant_retrieved_docs + 1 if test_mat[u,int(top_50_list[u,k])] == 1 else relevant_retrieved_docs\n",
        "    recall5[u,0] = recall_func(relevant_retrieved_docs,total_relevant[u])\n",
        "    precision5[u,0] = precision_func(relevant_retrieved_docs,5)\n",
        "    relevant_retrieved_docs = 0\n",
        "    #calculation for k = 20\n",
        "    for k in range(20):\n",
        "      relevant_retrieved_docs = relevant_retrieved_docs + 1 if test_mat[u,int(top_50_list[u,k])] == 1 else relevant_retrieved_docs\n",
        "    recall20[u,0] = recall_func(relevant_retrieved_docs,total_relevant[u])\n",
        "    precision20[u,0] = precision_func(relevant_retrieved_docs,20)\n",
        "    relevant_retrieved_docs = 0\n",
        "    #calculation for k = 50\n",
        "    for k in range(50):\n",
        "      relevant_retrieved_docs = relevant_retrieved_docs + 1 if test_mat[u,int(top_50_list[u,k])] == 1 else relevant_retrieved_docs\n",
        "    recall50[u,0] = recall_func(relevant_retrieved_docs,total_relevant[u])\n",
        "    precision50[u,0] = precision_func(relevant_retrieved_docs, 50)\n",
        "\n",
        "\n",
        "  print('Average Recall values for k = 5 is', (recall5.sum(axis = 0))/num_user)\n",
        "  print('Average Recall values for k = 20 is', (recall20.sum(axis = 0))/num_user)\n",
        "  print('Average Recall values for k = 50 is', (recall50.sum(axis = 0))/num_user)\n",
        "\n",
        "  print('Average Precision values for k = 5 is', (precision5.sum(axis = 0))/num_user)\n",
        "  print('Average Precision values for k = 20 is', (precision20.sum(axis = 0))/num_user)\n",
        "  print('Average Precision values for k = 50 is', (precision50.sum(axis = 0))/num_user)\n",
        "\n",
        "calculate_parameters(top_50_list, test_mat)\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qxAu2Gi_fU7m"
      },
      "source": [
        "# Part 2. Building the Collaborative Filtering Model for Implicit Feedback \n",
        "\n",
        "In this part, we will need to build **personalized** models instead of non-personalized models as in Part 1. We study how collaborative filtering algorithms work for recommendations with implicit feedback. The overall pipeline is the same as in Part 1. But now you need to implement a user-user collaborative filtering algorithm for recommendation. You will also evaluate your personalized models to compare them with the non-personalized one in Part 1.\n",
        "\n",
        "In this part, you will use the same MovieLens 1M dataset, and:\n",
        "\n",
        "* write the code to implement a user-user collaborative filtering algorithm,\n",
        "* and evaluate your recommender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs9uZydYfU7k"
      },
      "source": [
        "First, we need to load and preprocess the experiment dataset. We still use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4Y-6bhZfU7m"
      },
      "source": [
        "Then, you need to write code to implement a user-user collaborative filtering algorithm with **cosine similarity**. Although we have not discussed in class how to use collaborative filtering for implicit feedback, it is quite straightforward. \n",
        "\n",
        "* We first need to calculate the cosine similarity between users based on the binary feedback vectors of users; \n",
        "* Then, for a specific user $u$, we predict a preference vector for the user $u$ by weighted averaging the binary feedback vectors of N users who are most similar to $u$;\n",
        "* And last, rank the movies based on the predicted preference vector of the user $u$ as recommendations. \n",
        "\n",
        "The predicted preference score from user $u$ to movie $i$ can be calculated as: $p_{u,i}=\\frac{\\sum_{u^\\prime\\in N}s(u,u^\\prime)r_{u^\\prime,i}}{\\sum_{u^\\prime\\in N}|s(u,u^\\prime)|}$, where $s(u,u^\\prime)$ is the cosine similarity, and we set the size of $N$ as 10.\n",
        "\n",
        "In the next cell, write your code to generate the ranked lists of movies by this user-user collaborative filtering recommendation algorithm for every user, store the result in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. \n",
        "\n",
        "* Hint: for a user $u$, the movies user $u$ liked in the train_mat should be excluded in the top 50 recommendation list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfh815eYfU7m",
        "outputId": "094eb62c-e0c0-4402-d964-d45edea3e1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[207.  10. 390. 104. 381.]\n",
            "\n",
            "Top 5 recommended movies for first user are:\n",
            "Movie ID: 207.0 is at 1 position\n",
            "Movie ID: 10.0 is at 2 position\n",
            "Movie ID: 390.0 is at 3 position\n",
            "Movie ID: 104.0 is at 4 position\n",
            "Movie ID: 381.0 is at 5 position\n"
          ]
        }
      ],
      "source": [
        "# Your Code Here...\n",
        "train_mat\n",
        "test_mat\n",
        "cosine_mat = np.zeros((num_user,num_user))\n",
        "top_50_CF = np.zeros((num_user,50))\n",
        "for UserID in range(num_user):\n",
        "  for user_comp in range(num_user):\n",
        "    if UserID != user_comp:\n",
        "      cosine_mat[UserID][user_comp] = (np.dot(train_mat[UserID], train_mat[user_comp])/(np.linalg.norm(train_mat[UserID])*np.linalg.norm(train_mat[user_comp])))\n",
        "# print(cosine_mat.shape)\n",
        "# print(cosine_mat)\n",
        "\n",
        "for index, user in enumerate(train_mat):\n",
        "  knn_index = cosine_mat[index].argsort()[:-11:-1]\n",
        "  knn_simi_score = cosine_mat[index][knn_index]\n",
        "  preferance_vector = np.dot(knn_simi_score, train_mat[knn_index]) / knn_simi_score.sum()\n",
        "  watched_movies = np.where(train_mat[index,:]== 1)[0]\n",
        "  preferance_vector[watched_movies] = 0\n",
        "  top_50_CF[index] = preferance_vector.argsort()[:-51:-1]\n",
        "\n",
        "print(top_50_CF[0,:5])\n",
        "\n",
        "\n",
        "print('\\nTop 5 recommended movies for first user are:')\n",
        "for i in range(5):\n",
        "  print('Movie ID:', top_50_CF[0,i], 'is at', i+1, 'position')\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v73XLfN5fU7m"
      },
      "source": [
        "Last, you need to evaluate your user-user collaborative filtering algorithm by the held-out testing dataset test_mat for each user. Here, we use the same metrics recall@k and precision@k we used in Part 2c of Module 1. So you can use the same code here to calculate recall@k and precision@k for k=5, 20, 50 for each user, i.e., six metrics for every user. And please print out the average over all users for these six metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UI-iWnLfU7m",
        "outputId": "75c7dd3d-7cb0-45d9-bc84-70958745a622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[15 35 13 ...  6 41 98]\n",
            "Average Recall values for k = 5 is [0.07063241]\n",
            "Average Recall values for k = 20 is [0.191786]\n",
            "Average Recall values for k = 50 is [0.32227182]\n",
            "Average Precision values for k = 5 is [0.37821192]\n",
            "Average Precision values for k = 20 is [0.29107616]\n",
            "Average Precision values for k = 50 is [0.22180132]\n"
          ]
        }
      ],
      "source": [
        "# Calculate recall@k, precision@k with k=5, 20, 50 and print out the average over all users for these 6 metrics.\n",
        "# Your Code Here...\n",
        "\n",
        "calculate_parameters(top_50_CF, test_mat)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NORjV7c4P-V0"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh07jVftmBAY"
      },
      "source": [
        "**Question (5 points):** do you observe a better result compared with models implemented in the previous part? What's reason do you think that brings this improvement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6en1Xna8mBAZ"
      },
      "source": [
        "User-user collaborative filtering give better result than non-personalized recommender because it consider the individual preferences of each user. Non-personalized recommenders recommends the item by popularity and do not consider the unique preferences of individual user for which recommendation was made."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tl2tFlibnAY"
      },
      "source": [
        "# Part 3. Building the Matrix Factorization Model for Implicit Feedback \n",
        "\n",
        "Now we turn to study how matrix factorization algorithms works for recommendations with implicit feedback. The overall pipeline is the same as in Part 1 and Part 2. But now you need to implement a matrix factorization algorithm for recommendation. You will also compare your matrix factorization model to the non-personalized one in Part 1 and collaborative filtering one in Part 2.\n",
        "\n",
        "In this part, you will use the same MovieLens 1M dataset, and:\n",
        "* write the code to implement a matrix factorization algorithm,\n",
        "* and evaluate your recommender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTLaMD2CbnAS"
      },
      "source": [
        "First, let's recap the matrix factorization model introduced in class. The MF model can be mathematically represented as: \n",
        "\n",
        "<center>$\\underset{\\mathbf{P},\\mathbf{Q}}{\\text{min}}\\,\\,L=\\sum_{(u,i)\\in\\mathcal{O}}(\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-r_{u,i})^2+\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$,</center>\n",
        "    \n",
        "where $\\mathbf{P}$ is the user latent factor matrix of size (#user, #latent); $\\mathbf{Q}$ is the movie latent factor matrix of size (#movie, #latent); $\\mathcal{O}$ is a user-movie pair set containing all user-movie pairs having ratings in train_mat; $r_{u,i}$ represents the rating for user u and movie i; $\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$ is the regularization term to overcome overfitting problem, $\\lambda$ is the regularization weight (a hyper-parameter manually set by developer, i.e., you), and $\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{P}_{x,y})^2$, $\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{Q}_{x,y})^2$. Such an L function is called the **loss function** for the matrix factorization model. The goal of training an MF model is to find appropriate $\\mathbf{P}$ and $\\mathbf{Q}$ to minimize the loss L."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po4byk_1bnAZ"
      },
      "source": [
        "Although we have not discussed in class how to use matrix factorization algorithm for implicit feedback, it is quite straightforward. The main idea is the same as the MF model for explicit ratings. But instead of predicting explicit ratings, here the MF is to predict binary ratings based on which movies are ranked for users.\n",
        "\n",
        "The only challenge now is that in an implicit feedback dataset, there is only positive signal (i.e., '1' in the train_mat) without negative signal. Hence, to let our MF work for this implicit feedback, a simple but powerful method -- random negative sampling -- is adopted. The main idea is that in each training epoch, we randomly sample user-movie pairs without positive feedback in train_mat (user-movie pairs with '0' in train_mat) to be negative feedback, and mix them with positive feedback as the training data to train the MF model. The **negative sampling method is already provided**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-YYpBikbnAZ"
      },
      "source": [
        "In the Next cell, you need to complete the MF_implicit class. There are five functions in this class: \n",
        "\n",
        "* 'init' (**provided**) is to initialize the variables the MF class needs, which takes 5 inputs: train_mat, test_mat, latent, lr, and reg. 'train_mat' and 'test_mat' are the corresponfing training and testing matrices we have. 'latent' represents the latent dimension we set for the MF model. 'lr' represents the learning rate, i.e., the update step in each optimization iteration, default is 0.01. 'reg' represents the regularization weight, i.e., the $\\lambda$ in the MF formulation. \n",
        "\n",
        "* 'negative_sampling' (**provided**) is to do the random negative sampling to generate negative signals for training the MF. It returns a list of users and a list of movies as the training samples, mixing positive and negative user-movie pairs.\n",
        "\n",
        "* 'test' (**provided**) is to evaluate the trained MF on test_mat and print out recall@k and precision@k. \n",
        "\n",
        "* 'predict' (**need to be completed**) is to generate the ranked lists of movies by the trained MF for every user, store the ranking result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
        "\n",
        "* 'train' (**need to be completed**) is to train the MF model. There is only one input to this function: an int variable 'epoch' to indicate how many epochs for training the model. The main logic of this function is the same as the one you already implemented in Part 1a. The main body of this function should be a loop for 'epoch' iterations. In each iteration, following the algorithm to update the MF model:\n",
        "\n",
        "        1. Call 'negative_sampling()' to generate a list of users and a list of movies mixing positive and negative user-movie pairs as training samples.\n",
        "        2. Randomly shuffle training user-movie pairs  (i.e., user-movie pairs from step 1)\n",
        "        2. Have an inner loop to iterate each user-movie pair:\n",
        "                a. given a user-movie pair (u,i), update the user latent factor and movie latent factor by gradient decsent:    \n",
        "<center>$\\mathbf{P}_u=\\mathbf{P}_u-\\gamma [2(\\mathbf{P}_u\\cdot\\mathbf{Q}_i^\\top-r_{u,i})\\cdot\\mathbf{Q}_i+2\\lambda\\mathbf{P}_u]$</center>    \n",
        "<center>$\\mathbf{Q}_i=\\mathbf{Q}_i-\\gamma [2(\\mathbf{P}_u\\cdot\\mathbf{Q}_i^\\top-r_{u,i})\\cdot\\mathbf{P}_u+2\\lambda\\mathbf{Q}_i]$</center>    \n",
        "<center>where $\\mathbf{P}_u$ and $\\mathbf{Q}_i$ are row vectors of size (1, #latent), $\\gamma$ is learning rate (default is 0.01), $\\lambda$ is regularization weight, and $r_{u,i}$ now takes binary value.</center>\n",
        "        \n",
        "        3. After iterating over all user-movie pairs, call 'test()' to evaluate the current MF model.\n",
        "\n",
        "**Note: you are not supposed to delete or modify the provided code.**\n",
        "\n",
        "**Note: for this part, it is necessary to read and understand the provided code, because you will need to call the provided functions in your code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "NCQi55FTbnAa"
      },
      "outputs": [],
      "source": [
        "class MF_implicit:\n",
        "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
        "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
        "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
        "        \n",
        "        self.latent = latent  # the latent dimension\n",
        "        self.lr = lr  # learning rate\n",
        "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
        "        \n",
        "        self.num_user, self.num_movie = train_mat.shape\n",
        "        \n",
        "        self.sample_user, self.sample_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
        "        self.num_sample = len(self.sample_user)  # the number of user-movie pairs having ratings in train_mat\n",
        "\n",
        "        self.user_test_like = []\n",
        "        for u in range(self.num_user):\n",
        "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
        "\n",
        "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
        "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
        "        \n",
        "    def negative_sampling(self):\n",
        "        negative_movie = np.random.choice(np.arange(self.num_movie), size=(len(self.sample_user)), replace=True)\n",
        "        true_negative = self.train_mat[self.sample_user, negative_movie] == 0\n",
        "        negative_user = self.sample_user[true_negative]\n",
        "        negative_movie = negative_movie[true_negative]\n",
        "        return np.concatenate([self.sample_user, negative_user]), np.concatenate([self.sample_movie, negative_movie])\n",
        "\n",
        "    def train(self, epoch=20):\n",
        "        \"\"\"\n",
        "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
        "        Input: epoch -- the number of training epoch \n",
        "        \"\"\"\n",
        "        for ep in range(epoch):\n",
        "            \"\"\" \n",
        "            Write your code here to implement the training process for one epoch, \n",
        "            at the end of each epoch, run self.test() to evaluate current version of MF.\n",
        "            \"\"\"\n",
        "            user_indices, movie_indices = self.negative_sampling()\n",
        "            user_movie_pairs = list(zip(user_indices, movie_indices))\n",
        "\n",
        "            np.random.shuffle(user_movie_pairs)\n",
        "            \n",
        "            for user, movie in user_movie_pairs:\n",
        "                r_user_movie = self.train_mat[user, movie]\n",
        "                estimate = np.dot(self.P[user], self.Q[movie].T) - r_user_movie\n",
        "                self.P[user] = self.P[user] - self.lr * 2 * (estimate * self.Q[movie] + self.reg * self.P[user])\n",
        "                self.Q[movie] = self.Q[movie] - self.lr * 2 * (estimate * self.P[user] + self.reg * self.Q[movie])\n",
        "            \n",
        "            self.test()\n",
        "            \n",
        "            \"\"\"\n",
        "            End of your code for this function\n",
        "            \"\"\"\n",
        "            \n",
        "    def predict(self):\n",
        "        \"\"\"\n",
        "        Write your code here to implement the prediction function, which generates the ranked lists of movies \n",
        "        by the trained MF for every user, store the result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) \n",
        "        represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
        "        \"\"\"\n",
        "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
        "        recommendation = []\n",
        "        for u in range(self.num_user):\n",
        "            scores = prediction_mat[u]\n",
        "            train_like = np.where(train_mat[u, :] > 0)[0]\n",
        "            scores[train_like] = -9999\n",
        "            top50_iid = np.argpartition(scores, -50)[-50:]\n",
        "            top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
        "            recommendation.append(top50_iid)\n",
        "        recommendation = np.array(recommendation)\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        End of your code for this function\n",
        "        \"\"\"\n",
        "        return recommendation\n",
        "    \n",
        "    def test(self):\n",
        "        recommendation = self.predict()\n",
        "\n",
        "        recalls = np.zeros(3)\n",
        "        precisions = np.zeros(3)\n",
        "        user_count = 0.\n",
        "\n",
        "        for u in range(self.num_user):\n",
        "            test_like = self.user_test_like[u]\n",
        "            test_like_num = len(test_like)\n",
        "            if test_like_num == 0:\n",
        "                continue\n",
        "            rec = recommendation[u, :]\n",
        "            hits = np.zeros(3)\n",
        "            for k in range(50):\n",
        "                if rec[k] in test_like:\n",
        "                    if k < 50:\n",
        "                        hits[2] += 1\n",
        "                        if k < 20:\n",
        "                            hits[1] += 1\n",
        "                            if k < 5:\n",
        "                                hits[0] += 1\n",
        "            recalls[0] += (hits[0] / test_like_num)\n",
        "            recalls[1] += (hits[1] / test_like_num)\n",
        "            recalls[2] += (hits[2] / test_like_num)\n",
        "            precisions[0] += (hits[0] / 5.)\n",
        "            precisions[1] += (hits[1] / 20.)\n",
        "            precisions[2] += (hits[2] / 50.)\n",
        "            user_count += 1\n",
        "\n",
        "        recalls /= user_count\n",
        "        precisions /= user_count\n",
        "\n",
        "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
        "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK9lahvXbnAa"
      },
      "source": [
        "Now, run the next cell to build and train your implemented MF model for implicit feedback. The expected time used for training one epoch is 20s to 2 min."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1K6sLgEbnAa",
        "outputId": "e185657c-7442-429c-b509-eec23b55df59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall@5\t[0.012573],\t||\t recall@20\t[0.053551],\t||\t recall@50\t[0.130694]\n",
            "precision@5\t[0.135000],\t||\t precision@20\t[0.133626],\t||\t precision@50\t[0.123311]\n",
            "\n",
            "recall@5\t[0.022999],\t||\t recall@20\t[0.079590],\t||\t recall@50\t[0.161676]\n",
            "precision@5\t[0.207583],\t||\t precision@20\t[0.175017],\t||\t precision@50\t[0.143387]\n",
            "\n",
            "recall@5\t[0.030318],\t||\t recall@20\t[0.089967],\t||\t recall@50\t[0.173460]\n",
            "precision@5\t[0.238311],\t||\t precision@20\t[0.186697],\t||\t precision@50\t[0.148815]\n",
            "\n",
            "recall@5\t[0.025698],\t||\t recall@20\t[0.090851],\t||\t recall@50\t[0.175967]\n",
            "precision@5\t[0.219636],\t||\t precision@20\t[0.192368],\t||\t precision@50\t[0.150758]\n",
            "\n",
            "recall@5\t[0.030231],\t||\t recall@20\t[0.092757],\t||\t recall@50\t[0.176716]\n",
            "precision@5\t[0.246291],\t||\t precision@20\t[0.197053],\t||\t precision@50\t[0.154728]\n",
            "\n",
            "recall@5\t[0.026619],\t||\t recall@20\t[0.087355],\t||\t recall@50\t[0.179300]\n",
            "precision@5\t[0.226556],\t||\t precision@20\t[0.196846],\t||\t precision@50\t[0.163692]\n",
            "\n",
            "recall@5\t[0.027956],\t||\t recall@20\t[0.094028],\t||\t recall@50\t[0.185375]\n",
            "precision@5\t[0.251987],\t||\t precision@20\t[0.211134],\t||\t precision@50\t[0.171093]\n",
            "\n",
            "recall@5\t[0.030485],\t||\t recall@20\t[0.097819],\t||\t recall@50\t[0.192814]\n",
            "precision@5\t[0.259834],\t||\t precision@20\t[0.219901],\t||\t precision@50\t[0.179738]\n",
            "\n",
            "recall@5\t[0.031869],\t||\t recall@20\t[0.103064],\t||\t recall@50\t[0.202292]\n",
            "precision@5\t[0.274172],\t||\t precision@20\t[0.227285],\t||\t precision@50\t[0.185166]\n",
            "\n",
            "recall@5\t[0.032499],\t||\t recall@20\t[0.105111],\t||\t recall@50\t[0.209195]\n",
            "precision@5\t[0.277649],\t||\t precision@20\t[0.231267],\t||\t precision@50\t[0.189911]\n",
            "\n",
            "recall@5\t[0.032046],\t||\t recall@20\t[0.106421],\t||\t recall@50\t[0.213757]\n",
            "precision@5\t[0.273808],\t||\t precision@20\t[0.231863],\t||\t precision@50\t[0.191709]\n",
            "\n",
            "recall@5\t[0.034159],\t||\t recall@20\t[0.110843],\t||\t recall@50\t[0.219155]\n",
            "precision@5\t[0.279536],\t||\t precision@20\t[0.237177],\t||\t precision@50\t[0.194944]\n",
            "\n",
            "recall@5\t[0.034637],\t||\t recall@20\t[0.112718],\t||\t recall@50\t[0.221682]\n",
            "precision@5\t[0.281523],\t||\t precision@20\t[0.237219],\t||\t precision@50\t[0.195139]\n",
            "\n",
            "recall@5\t[0.035455],\t||\t recall@20\t[0.114273],\t||\t recall@50\t[0.226696]\n",
            "precision@5\t[0.284338],\t||\t precision@20\t[0.239503],\t||\t precision@50\t[0.197185]\n",
            "\n",
            "recall@5\t[0.035348],\t||\t recall@20\t[0.115984],\t||\t recall@50\t[0.230498]\n",
            "precision@5\t[0.281325],\t||\t precision@20\t[0.238212],\t||\t precision@50\t[0.197646]\n",
            "\n",
            "recall@5\t[0.037244],\t||\t recall@20\t[0.118582],\t||\t recall@50\t[0.232692]\n",
            "precision@5\t[0.290497],\t||\t precision@20\t[0.242674],\t||\t precision@50\t[0.198702]\n",
            "\n",
            "recall@5\t[0.036935],\t||\t recall@20\t[0.119520],\t||\t recall@50\t[0.235736]\n",
            "precision@5\t[0.285530],\t||\t precision@20\t[0.241275],\t||\t precision@50\t[0.198887]\n",
            "\n",
            "recall@5\t[0.036982],\t||\t recall@20\t[0.119845],\t||\t recall@50\t[0.238936]\n",
            "precision@5\t[0.285066],\t||\t precision@20\t[0.242475],\t||\t precision@50\t[0.200596]\n",
            "\n",
            "recall@5\t[0.037306],\t||\t recall@20\t[0.121571],\t||\t recall@50\t[0.239883]\n",
            "precision@5\t[0.285762],\t||\t precision@20\t[0.242972],\t||\t precision@50\t[0.200540]\n",
            "\n",
            "recall@5\t[0.038022],\t||\t recall@20\t[0.122397],\t||\t recall@50\t[0.240929]\n",
            "precision@5\t[0.288311],\t||\t precision@20\t[0.242575],\t||\t precision@50\t[0.200447]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "mf_implicit = MF_implicit(train_mat, test_mat, latent=5, lr=0.01, reg=0.0001)\n",
        "mf_implicit.train(epoch=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_IrnxJl961"
      },
      "source": [
        "**Question (5 points):** do you observe a better result compared with models implemented in previous two parts? What's reason do you think that brings this improvement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxwdsGYel961"
      },
      "source": [
        "Usually MF should give better recall and precision as compared to User-user collaborative filtering due to following reason:\n",
        "\n",
        "* It can handle large-scale data as it convert it into smaller matrices of user and item latent factors. This makes the computation more efficient and faster.\n",
        "* It can make recommendations for new users or items based on their latent factors which it learn from existing user-item interactions.\n",
        "* The model could easily capture and learn the underlying patterns in the data which would be hard for methods like user-user CF.\n",
        "\n",
        "But we can observe above that the recall and precision values achieved are less as compared to the user-user CF. This could be beacuse of following reasons:\n",
        "\n",
        "* We can see that the used user-movie matrix is approx 89% empty and is sparse so it can be difficult for MF to learn meaningful representations of users and items. In such cases, user-user collaborative filtering can be a better option as it relies on the similarity between users.\n",
        "\n",
        "* Currently the model is using implicit feedback data which do not provide much details about item and is inherently noisy.\n",
        "\n",
        "* Inadequate hypermeter tuning - currently we are using the given hyperparameters values - number of latent factors = 5, Learning rate = 0.01 and regularization factor 0.001. It might be possible that model is getting underfit or overfit for these particular hyperparameters and require further tuning.\n",
        "\n",
        "* It might be a case that data is highly skewed towards certain items or certain types of users which makes it diffcult for model to make accurate recommendations for a diverse set of users and items."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4PW9KO1gMD-e"
      },
      "source": [
        "# Part 4: Building the Bayesian Personalized Ranking (BPR) Model with Implicit Feedback \n",
        "\n",
        "In this first part, you will need to build a **Bayesian Personalized Ranking (BPR)** model. You will also evaluate your BPR model to compare with the non-personalized one in Part 1, neighborhood-based collaborative filtering model in Part 2, and matrix factorization model in Part 3. \n",
        "\n",
        "For this part, we will:\n",
        "\n",
        "* build a BPR model,\n",
        "* and evaluate your recommender."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-hWSeW5MD-e"
      },
      "source": [
        "We still use the MovieLens 1M data from https://grouplens.org/datasets/movielens/1m/ in this part, and use the same loading and preprocessing process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO69amdAMD-k"
      },
      "source": [
        "First, let's implement the Bayesian Personalized Ranking model introduced in class. The BPR model can be mathematically represented as: \n",
        "\n",
        "<center>$\\underset{\\mathbf{P},\\mathbf{Q}}{\\text{max}}\\,\\,L=\\sum_{(u,i,j)\\in\\mathcal{O}}\\sigma(\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_j)+\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$,</center>\n",
        "    \n",
        "where $\\mathbf{P}$ is the user latent factor matrix of size (#user, #latent); $\\mathbf{Q}$ is the movie latent factor matrix of size (#movie, #latent); $\\sigma(\\cdot)$ is the Sigmoid function; $\\mathcal{O}$ is a (user, positive movie, negative movie) tuple set, and each tuple $(u,i,j)$ is a training sample with user $u$ and a posotive movie $i$ with value 1 in train_mat and a negative movie $j$ with value 0 in train_mat; $\\lambda(\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}+\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}})$ is the regularization term to overcome overfitting problem, $\\lambda$ is the regularization weight (a hyper-parameter manually set by the developer, i.e., you), and $\\lVert\\mathbf{P}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{P}_{x,y})^2$, $\\lVert\\mathbf{Q}\\rVert^2_{\\text{F}}=\\sum_{x}\\sum_{y}(\\mathbf{Q}_{x,y})^2$. Such an L function is called the **loss function** for the matrix factorization model. The goal of training a BPR model is to find appropriate $\\mathbf{P}$ and $\\mathbf{Q}$ to **maximize** the loss L."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kpemLFvMD-l"
      },
      "source": [
        "To implement such a BPR model, here we will write a Python class for the model. Similar to the implicit_MF model you have already implemented in Part 3, there are five functions in this BPR class: \n",
        "\n",
        "* The 'init' function (**provided**) is to initialize the variables the BPR class needs, which takes 5 inputs: train_mat, test_mat, latent, lr, and reg. 'train_mat' and 'test_mat' are the corresponfing training and testing matrices we have. 'latent' represents the latent dimension we set for the BPR model. 'lr' represents the learning rate, i.e., the update step in each optimization iteration, default is 0.01. 'reg' represents the regularization weight, i.e., the $\\lambda$ in the BPR formulation.\n",
        "\n",
        "* 'negative_sampling' (**provided**) is to do the random negative sampling to generate negative signals for training the BPR. It returns a list of users, a list of positive movies of these users, and a list of negative movies of these user. These three lists form the training set $\\mathcal{O}$.\n",
        "\n",
        "* 'test' (**provided**) is to evaluate the trained MF on test_mat and print out recall@k and precision@k. \n",
        "\n",
        "* 'predict' (**provided**) is to generates the ranked lists of movies by the trained MF for every user, store the ranking result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
        "\n",
        "* 'train' (**need to be completed**) is to train the BPR model. There is only one input to this function: an int variable 'epoch' to indicate how many epochs for training the model. The main process of one training epoch is:\n",
        "\n",
        "        1. Call 'negative_sampling()' to generate training samples: a list of users, a list of positive movies of these users, and a list of negative movies of these users.\n",
        "        2. Randomly shuffle training samples from step 1.\n",
        "        3. Have an inner loop to iterate each (user, positive movie, negative movie) tuple in the shuffled training samples:\n",
        "                a. given the current training tuple -- a user u, a positive movie i, and a negative movie j, update the user latent factor and movie latent factor by gradient decsent:\n",
        "<center>$\\mathbf{P}_u=\\mathbf{P}_u - \\gamma[-\\frac{e^{-\\widehat{x}}}{1 + e^{-\\widehat{x}}} (\\mathbf{Q}_i - \\mathbf{Q}_j) + \\lambda\\mathbf{P}_u]$</center>\n",
        "<center>$\\mathbf{Q}_i=\\mathbf{Q}_i - \\gamma[-\\frac{e^{-\\widehat{x}}}{1 + e^{-\\widehat{x}}} \\mathbf{P}_U + \\lambda\\mathbf{Q}_i]$</center>\n",
        "<center>$\\mathbf{Q}_j=\\mathbf{Q}_j - \\gamma[\\frac{e^{-\\widehat{x}}}{1 + e^{-\\widehat{x}}} \\mathbf{P}_U + \\lambda\\mathbf{Q}_j]$</center>   \n",
        "<center>where $\\mathbf{P}_u$, $\\mathbf{Q}_i$, and $\\mathbf{Q}_j$ are of size (1, #latent), $\\gamma$ is learning rate (default is 0.01), $\\lambda$ is regularization weight, and $\\widehat{x}=\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_i-\\mathbf{P}_u\\cdot\\mathbf{Q}^\\top_j$.</center>\n",
        "        \n",
        "        3. After iterate all training samples, call 'test()' to evaluate the current BPR model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYQmt-3wMD-l"
      },
      "source": [
        "In the next cell, you will need to fill in the 'train' function based on the description above. \n",
        "\n",
        "*Hint that similar to the training process of MF in Part 3, in each update iteration, you need to use the old P and Q from last update iteration to update P and Q in current update iteration.\n",
        "\n",
        "**NOTE that you should not delete or modify the provided code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "R7oKJUR9MD-l"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class BPR:\n",
        "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
        "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
        "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
        "        \n",
        "        self.latent = latent  # the latent dimension\n",
        "        self.lr = lr  # learning rate\n",
        "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
        "        \n",
        "        self.num_user, self.num_movie = train_mat.shape\n",
        "        \n",
        "        self.positive_user, self.positive_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
        "\n",
        "        self.user_test_like = []\n",
        "        for u in range(self.num_user):\n",
        "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
        "\n",
        "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
        "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
        "        \n",
        "    def negative_sampling(self): \n",
        "        # do the negative sampling for each of the positive user-movie pair. Here we set negative sampling rate as 2, \n",
        "        # i.e., for each positive user-movie pair, randomly sample two negative movies. \n",
        "        # This function returns final training data: a user list, a positive movie list, and a negative movie list\n",
        "        sample_user = np.tile(self.positive_user, 2)\n",
        "        sample_pos_movie = np.tile(self.positive_movie, 2)\n",
        "        sample_neg_movie = np.random.choice(np.arange(self.num_movie), size=(len(sample_user)), replace=True)\n",
        "        true_negative = self.train_mat[sample_user, sample_neg_movie] == 0\n",
        "        return sample_user[true_negative], sample_pos_movie[true_negative], sample_neg_movie[true_negative]\n",
        "\n",
        "    def train(self, epoch=20):\n",
        "        \"\"\"\n",
        "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
        "        Input: epoch -- the number of training epoch \n",
        "        \"\"\"\n",
        "        for ep in range(epoch):\n",
        "            \"\"\" \n",
        "            Write your code here to implement the training process for one epoch, \n",
        "            at the end of each epoch, run self.test() to evaluate current version of BPR.\n",
        "            \"\"\"\n",
        "            user_list, pos_list, neg_list = self.negative_sampling()\n",
        "            uij_samples = list(zip(user_list, pos_list, neg_list))\n",
        "            np.random.shuffle(uij_samples)\n",
        "            for user, pos_item, neg_item in uij_samples:\n",
        "                \n",
        "                pos_latent = self.Q[pos_item]\n",
        "                neg_latent = self.Q[neg_item]\n",
        "                user_latent = self.P[user]\n",
        "                x_hat = np.dot(user_latent, pos_latent - neg_latent)\n",
        "                \n",
        "                sigmoid_x_hat = math.exp(-x_hat) / (1 + math.exp(-x_hat))\n",
        "\n",
        "                self.P[user] -= self.lr * (-sigmoid_x_hat * (pos_latent - neg_latent) + self.reg * user_latent)\n",
        "                self.Q[pos_item] -= self.lr * (-sigmoid_x_hat * user_latent + self.reg * pos_latent)\n",
        "                self.Q[neg_item] -= self.lr * (sigmoid_x_hat * user_latent + self.reg * neg_latent)\n",
        "\n",
        "            self.test()\n",
        "            \"\"\"\n",
        "            End of your code for this function\n",
        "            \"\"\"\n",
        "            \n",
        "    def predict(self):\n",
        "        # The prediction function, which generates the ranked lists of movies \n",
        "        # by the trained BPR for every user, store the result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) \n",
        "        # represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
        "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
        "        recommendation = []\n",
        "        for u in range(self.num_user):\n",
        "            scores = prediction_mat[u]\n",
        "            train_like = np.where(train_mat[u, :] > 0)[0]\n",
        "            scores[train_like] = -9999\n",
        "            top50_iid = np.argpartition(scores, -50)[-50:]\n",
        "            top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
        "            recommendation.append(top50_iid)\n",
        "        recommendation = np.array(recommendation)\n",
        "        return recommendation\n",
        "    \n",
        "    def test(self):\n",
        "        recommendation = self.predict()\n",
        "\n",
        "        recalls = np.zeros(3)\n",
        "        precisions = np.zeros(3)\n",
        "        user_count = 0.\n",
        "\n",
        "        for u in range(self.num_user):\n",
        "            test_like = self.user_test_like[u]\n",
        "            test_like_num = len(test_like)\n",
        "            if test_like_num == 0:\n",
        "                continue\n",
        "            rec = recommendation[u, :]\n",
        "            hits = np.zeros(3)\n",
        "            for k in range(50):\n",
        "                if rec[k] in test_like:\n",
        "                    if k < 50:\n",
        "                        hits[2] += 1\n",
        "                        if k < 20:\n",
        "                            hits[1] += 1\n",
        "                            if k < 5:\n",
        "                                hits[0] += 1\n",
        "            recalls[0] += (hits[0] / test_like_num)\n",
        "            recalls[1] += (hits[1] / test_like_num)\n",
        "            recalls[2] += (hits[2] / test_like_num)\n",
        "            precisions[0] += (hits[0] / 5.)\n",
        "            precisions[1] += (hits[1] / 20.)\n",
        "            precisions[2] += (hits[2] / 50.)\n",
        "            user_count += 1\n",
        "\n",
        "        recalls /= user_count\n",
        "        precisions /= user_count\n",
        "\n",
        "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
        "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvJRalhwMD-r"
      },
      "source": [
        "Now, let's train a BPR model based on your implementation. The code is provided, you just need to execute the next cell. The expectations are: \n",
        "\n",
        "* first, the code can be successfully executed without error; \n",
        "* and second, the recall@k and precision@k on **test_mat** of each training epoch should be printed out for all 20 epochs.\n",
        "\n",
        "\n",
        "* Hint: the expected time used for training is around 1 minute to 5 minutes per training epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-q9CtdDMD-s",
        "outputId": "eb565235-fffb-4a2c-e701-8e08556132a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall@5\t[0.034968],\t||\t recall@20\t[0.105035],\t||\t recall@50\t[0.188376]\n",
            "precision@5\t[0.258477],\t||\t precision@20\t[0.206465],\t||\t precision@50\t[0.154798]\n",
            "\n",
            "recall@5\t[0.034815],\t||\t recall@20\t[0.105514],\t||\t recall@50\t[0.189177]\n",
            "precision@5\t[0.260166],\t||\t precision@20\t[0.206283],\t||\t precision@50\t[0.155222]\n",
            "\n",
            "recall@5\t[0.035541],\t||\t recall@20\t[0.105341],\t||\t recall@50\t[0.188762]\n",
            "precision@5\t[0.263278],\t||\t precision@20\t[0.206821],\t||\t precision@50\t[0.155361]\n",
            "\n",
            "recall@5\t[0.035530],\t||\t recall@20\t[0.105910],\t||\t recall@50\t[0.190542]\n",
            "precision@5\t[0.265099],\t||\t precision@20\t[0.207980],\t||\t precision@50\t[0.157023]\n",
            "\n",
            "recall@5\t[0.036079],\t||\t recall@20\t[0.106274],\t||\t recall@50\t[0.192402]\n",
            "precision@5\t[0.267384],\t||\t precision@20\t[0.210248],\t||\t precision@50\t[0.159563]\n",
            "\n",
            "recall@5\t[0.036504],\t||\t recall@20\t[0.107039],\t||\t recall@50\t[0.193903]\n",
            "precision@5\t[0.270728],\t||\t precision@20\t[0.212202],\t||\t precision@50\t[0.162331]\n",
            "\n",
            "recall@5\t[0.036495],\t||\t recall@20\t[0.108971],\t||\t recall@50\t[0.197809]\n",
            "precision@5\t[0.277119],\t||\t precision@20\t[0.218088],\t||\t precision@50\t[0.166421]\n",
            "\n",
            "recall@5\t[0.037021],\t||\t recall@20\t[0.111339],\t||\t recall@50\t[0.201020]\n",
            "precision@5\t[0.278907],\t||\t precision@20\t[0.221399],\t||\t precision@50\t[0.169632]\n",
            "\n",
            "recall@5\t[0.037501],\t||\t recall@20\t[0.112532],\t||\t recall@50\t[0.205322]\n",
            "precision@5\t[0.282947],\t||\t precision@20\t[0.225778],\t||\t precision@50\t[0.173881]\n",
            "\n",
            "recall@5\t[0.039108],\t||\t recall@20\t[0.113879],\t||\t recall@50\t[0.207829]\n",
            "precision@5\t[0.295530],\t||\t precision@20\t[0.230149],\t||\t precision@50\t[0.177669]\n",
            "\n",
            "recall@5\t[0.039926],\t||\t recall@20\t[0.116993],\t||\t recall@50\t[0.212928]\n",
            "precision@5\t[0.302020],\t||\t precision@20\t[0.234222],\t||\t precision@50\t[0.181652]\n",
            "\n",
            "recall@5\t[0.040257],\t||\t recall@20\t[0.118643],\t||\t recall@50\t[0.217794]\n",
            "precision@5\t[0.308311],\t||\t precision@20\t[0.239553],\t||\t precision@50\t[0.185735]\n",
            "\n",
            "recall@5\t[0.042094],\t||\t recall@20\t[0.121618],\t||\t recall@50\t[0.222695]\n",
            "precision@5\t[0.314040],\t||\t precision@20\t[0.244445],\t||\t precision@50\t[0.189742]\n",
            "\n",
            "recall@5\t[0.043405],\t||\t recall@20\t[0.124096],\t||\t recall@50\t[0.227682]\n",
            "precision@5\t[0.324371],\t||\t precision@20\t[0.249180],\t||\t precision@50\t[0.194033]\n",
            "\n",
            "recall@5\t[0.043865],\t||\t recall@20\t[0.126646],\t||\t recall@50\t[0.232363]\n",
            "precision@5\t[0.329636],\t||\t precision@20\t[0.254520],\t||\t precision@50\t[0.196695]\n",
            "\n",
            "recall@5\t[0.044501],\t||\t recall@20\t[0.128614],\t||\t recall@50\t[0.237305]\n",
            "precision@5\t[0.332384],\t||\t precision@20\t[0.257078],\t||\t precision@50\t[0.200076]\n",
            "\n",
            "recall@5\t[0.046355],\t||\t recall@20\t[0.131261],\t||\t recall@50\t[0.240760]\n",
            "precision@5\t[0.340298],\t||\t precision@20\t[0.260546],\t||\t precision@50\t[0.201987]\n",
            "\n",
            "recall@5\t[0.046411],\t||\t recall@20\t[0.133190],\t||\t recall@50\t[0.243745]\n",
            "precision@5\t[0.339073],\t||\t precision@20\t[0.262384],\t||\t precision@50\t[0.203397]\n",
            "\n",
            "recall@5\t[0.047290],\t||\t recall@20\t[0.134647],\t||\t recall@50\t[0.247921]\n",
            "precision@5\t[0.343344],\t||\t precision@20\t[0.263916],\t||\t precision@50\t[0.205026]\n",
            "\n",
            "recall@5\t[0.048200],\t||\t recall@20\t[0.137828],\t||\t recall@50\t[0.250164]\n",
            "precision@5\t[0.348079],\t||\t precision@20\t[0.267483],\t||\t precision@50\t[0.206136]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "bpr = BPR(train_mat, test_mat, latent=5, lr=0.01, reg=0.001)\n",
        "bpr.train(epoch=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ptxnml0-MD-s"
      },
      "source": [
        "**Question (5 points):** do you observe a better result compared with models implemented in previous three parts? What's reason do you think that brings this improvement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0M28QNVMD-s"
      },
      "source": [
        "BPR is giving better recall and precision as compared to MF due to following reason:\n",
        "\n",
        "* BPR is designed for implicit feedback. Absence of feedback indicates that the user is not interested in the item. But MF works better with explicit feedback where the user provides explicit ratings or feedback for the items. \n",
        "\n",
        "* In BPR the ranking of items is based on the pairwise comparisons of items by the user. This approach is more flexible than the approach used in MF which may not capture the complex relationships between the user and items.\n",
        "\n",
        "\n",
        "But we can observe above that the BPR recall and precision values achieved are less as compared to the user-user CF. This could be beacuse of following reasons:\n",
        "\n",
        "* We can see that the used user-movie matrix is approx 89% empty and is sparse so it can be difficult for BPR to learn meaningful representations of users and items. In such cases, user-user collaborative filtering can be a better option as it relies on the similarity between users.\n",
        "\n",
        "* Inadequate hypermeter tuning - currently we are using the given hyperparameters values - number of latent factors = 5, Learning rate = 0.01 and regularization factor 0.001. It might be possible that model is getting underfit or overfit for these particular hyperparameters and require further tuning.\n",
        "\n",
        "* It might be a case that data is highly skewed towards certain items or certain types of users which makes it diffcult for model to make accurate recommendations for a diverse set of users and items."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wKZ8ijNPTXMN"
      },
      "source": [
        "# Part 5. Cool Extension! \n",
        "\n",
        "Just like in the first homework, now is your chance to try an interesting extension. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0R08ZESyg93"
      },
      "source": [
        "I am adding user bias and item bias parameters to BPR. BPR will learn the user bias and item bias parameters on its own instead of explicitly coding it as we do in baseline estimate. \n",
        "\n",
        "Also I am using the movie popularity score and using it as the initial values for item bias which will add more weight to the movies that are famous and give less weightage to movies that are never watched during start and will help model to converge faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "01XLVpCArpOD"
      },
      "outputs": [],
      "source": [
        "class BPRwith_item_popularity_user_item_bias:\n",
        "    def __init__(self, train_mat, test_mat, latent=5, lr=0.01, reg=0.01):\n",
        "        self.train_mat = train_mat  # the training rating matrix of size (#user, #movie)\n",
        "        self.test_mat = test_mat  # the training rating matrix of size (#user, #movie)\n",
        "        \n",
        "        self.latent = latent  # the latent dimension\n",
        "        self.lr = lr  # learning rate\n",
        "        self.reg = reg  # regularization weight, i.e., the lambda in the objective function\n",
        "        \n",
        "        self.num_user, self.num_movie = train_mat.shape\n",
        "        \n",
        "        self.positive_user, self.positive_movie = self.train_mat.nonzero()  # get the user-movie paris having ratings in train_mat\n",
        "\n",
        "        self.user_test_like = []\n",
        "        for u in range(self.num_user):\n",
        "            self.user_test_like.append(np.where(self.test_mat[u, :] > 0)[0])\n",
        "\n",
        "        self.popularity = train_mat.sum(axis=0).astype(float)\n",
        "        z = np.where(self.popularity == 0)[0] \n",
        "        self.popularity[z] = 0.001\n",
        "        self.popularity = 1/np.log(1+self.popularity)\n",
        "        \n",
        "        self.bu = np.random.random((num_user,1))\n",
        "        self.bi = self.popularity\n",
        "        self.P = np.random.random((self.num_user, self.latent))  # latent factors for users, size (#user, self.latent), randomly initialized\n",
        "        self.Q = np.random.random((self.num_movie, self.latent))  # latent factors for users, size (#movie, self.latent), randomly initialized\n",
        "        \n",
        "    def negative_sampling(self): \n",
        "        # do the negative sampling for each of the positive user-movie pair. Here we set negative sampling rate as 2, \n",
        "        # i.e., for each positive user-movie pair, randomly sample two negative movies. \n",
        "        # This function returns final training data: a user list, a positive movie list, and a negative movie list\n",
        "        sample_user = np.tile(self.positive_user, 2)\n",
        "        sample_pos_movie = np.tile(self.positive_movie, 2)\n",
        "        sample_neg_movie = np.random.choice(np.arange(self.num_movie), size=(len(sample_user)), replace=True)\n",
        "        true_negative = self.train_mat[sample_user, sample_neg_movie] == 0\n",
        "        return sample_user[true_negative], sample_pos_movie[true_negative], sample_neg_movie[true_negative]\n",
        "\n",
        "    def train(self, epoch=20):\n",
        "        \"\"\"\n",
        "        Goal: Write your code to train your matrix factorization model for epoch iterations in this function\n",
        "        Input: epoch -- the number of training epoch \n",
        "        \"\"\"\n",
        "        \n",
        "        for ep in range(epoch):\n",
        "            \"\"\" \n",
        "            Write your code here to implement the training process for one epoch, \n",
        "            at the end of each epoch, run self.test() to evaluate current version of BPR.\n",
        "            \"\"\"\n",
        "            user_list, pos_list, neg_list = self.negative_sampling()\n",
        "            uij_samples = list(zip(user_list, pos_list, neg_list))\n",
        "            np.random.shuffle(uij_samples)\n",
        "            for user, pos_item, neg_item in uij_samples:\n",
        "                \n",
        "                pos_latent = self.Q[pos_item]\n",
        "                neg_latent = self.Q[neg_item]\n",
        "                user_latent = self.P[user]\n",
        "                x_hat = np.dot(user_latent, pos_latent - neg_latent) + self.bu[user] + self.bi[pos_item]\n",
        "                \n",
        "                sigmoid_x_hat = np.exp(-x_hat) / (1 + np.exp(-x_hat)) \n",
        "\n",
        "                self.P[user] -= self.lr * (-sigmoid_x_hat * (pos_latent - neg_latent) + self.reg * user_latent) \n",
        "                self.Q[pos_item] -= self.lr * (-sigmoid_x_hat * user_latent + self.reg * pos_latent)\n",
        "                self.Q[neg_item] -= self.lr * (sigmoid_x_hat * user_latent + self.reg * neg_latent)\n",
        "                self.bu[user] -= self.lr * (-sigmoid_x_hat + self.reg * self.bu[user])\n",
        "                self.bi[pos_item] -= self.lr * (-sigmoid_x_hat + self.reg * self.bi[pos_item])\n",
        "\n",
        "            self.test()\n",
        "            \"\"\"\n",
        "            End of your code for this function\n",
        "            \"\"\"\n",
        "            \n",
        "    def predict(self):\n",
        "        # The prediction function, which generates the ranked lists of movies \n",
        "        # by the trained BPR for every user, store the result (named 'recommendation') in a numpy array of size (#user, 50), where entry (u, k) \n",
        "        # represents the movie id that is ranked at position k in the recommendation list to user u. Return the 'recommendation' variable. \n",
        "        prediction_mat = np.matmul(self.P, self.Q.T)\n",
        "        recommendation = []\n",
        "        for u in range(self.num_user):\n",
        "            scores = prediction_mat[u]\n",
        "            train_like = np.where(train_mat[u, :] > 0)[0]\n",
        "            scores[train_like] = -9999\n",
        "            top50_iid = np.argpartition(scores, -50)[-50:]\n",
        "            top50_iid = top50_iid[np.argsort(scores[top50_iid])[-1::-1]]\n",
        "            recommendation.append(top50_iid)\n",
        "        recommendation = np.array(recommendation)\n",
        "        return recommendation\n",
        "    \n",
        "    def test(self):\n",
        "        recommendation = self.predict()\n",
        "\n",
        "        recalls = np.zeros(3)\n",
        "        precisions = np.zeros(3)\n",
        "        user_count = 0.\n",
        "\n",
        "        for u in range(self.num_user):\n",
        "            test_like = self.user_test_like[u]\n",
        "            test_like_num = len(test_like)\n",
        "            if test_like_num == 0:\n",
        "                continue\n",
        "            rec = recommendation[u, :]\n",
        "            hits = np.zeros(3)\n",
        "            for k in range(50):\n",
        "                if rec[k] in test_like:\n",
        "                    if k < 50:\n",
        "                        hits[2] += 1\n",
        "                        if k < 20:\n",
        "                            hits[1] += 1\n",
        "                            if k < 5:\n",
        "                                hits[0] += 1\n",
        "            recalls[0] += (hits[0] / test_like_num)\n",
        "            recalls[1] += (hits[1] / test_like_num)\n",
        "            recalls[2] += (hits[2] / test_like_num)\n",
        "            precisions[0] += (hits[0] / 5.)\n",
        "            precisions[1] += (hits[1] / 20.)\n",
        "            precisions[2] += (hits[2] / 50.)\n",
        "            user_count += 1\n",
        "\n",
        "        recalls /= user_count\n",
        "        precisions /= user_count\n",
        "\n",
        "        print('recall@5\\t[%.6f],\\t||\\t recall@20\\t[%.6f],\\t||\\t recall@50\\t[%.6f]' % (recalls[0], recalls[1], recalls[2]))\n",
        "        print('precision@5\\t[%.6f],\\t||\\t precision@20\\t[%.6f],\\t||\\t precision@50\\t[%.6f]' % (precisions[0], precisions[1], precisions[2]))\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiPEhKW7rpoE",
        "outputId": "cc8d0c7e-e107-4c82-8d8b-0b5eb0ad4445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "recall@5\t[0.026951],\t||\t recall@20\t[0.086781],\t||\t recall@50\t[0.164726]\n",
            "precision@5\t[0.217517],\t||\t precision@20\t[0.177873],\t||\t precision@50\t[0.139139]\n",
            "\n",
            "recall@5\t[0.028418],\t||\t recall@20\t[0.088146],\t||\t recall@50\t[0.166548]\n",
            "precision@5\t[0.226192],\t||\t precision@20\t[0.180157],\t||\t precision@50\t[0.140639]\n",
            "\n",
            "recall@5\t[0.029808],\t||\t recall@20\t[0.090299],\t||\t recall@50\t[0.169363]\n",
            "precision@5\t[0.233742],\t||\t precision@20\t[0.184685],\t||\t precision@50\t[0.142391]\n",
            "\n",
            "recall@5\t[0.029839],\t||\t recall@20\t[0.092974],\t||\t recall@50\t[0.170933]\n",
            "precision@5\t[0.235828],\t||\t precision@20\t[0.188303],\t||\t precision@50\t[0.143142]\n",
            "\n",
            "recall@5\t[0.031432],\t||\t recall@20\t[0.094359],\t||\t recall@50\t[0.172857]\n",
            "precision@5\t[0.243609],\t||\t precision@20\t[0.190828],\t||\t precision@50\t[0.144573]\n",
            "\n",
            "recall@5\t[0.032981],\t||\t recall@20\t[0.096859],\t||\t recall@50\t[0.175545]\n",
            "precision@5\t[0.251854],\t||\t precision@20\t[0.193990],\t||\t precision@50\t[0.145851]\n",
            "\n",
            "recall@5\t[0.032678],\t||\t recall@20\t[0.098962],\t||\t recall@50\t[0.177379]\n",
            "precision@5\t[0.251192],\t||\t precision@20\t[0.196871],\t||\t precision@50\t[0.146927]\n",
            "\n",
            "recall@5\t[0.033424],\t||\t recall@20\t[0.099542],\t||\t recall@50\t[0.179779]\n",
            "precision@5\t[0.254834],\t||\t precision@20\t[0.198270],\t||\t precision@50\t[0.148652]\n",
            "\n",
            "recall@5\t[0.034579],\t||\t recall@20\t[0.100946],\t||\t recall@50\t[0.182241]\n",
            "precision@5\t[0.259536],\t||\t precision@20\t[0.200306],\t||\t precision@50\t[0.150073]\n",
            "\n",
            "recall@5\t[0.034899],\t||\t recall@20\t[0.102391],\t||\t recall@50\t[0.184187]\n",
            "precision@5\t[0.259669],\t||\t precision@20\t[0.202210],\t||\t precision@50\t[0.151063]\n",
            "\n",
            "recall@5\t[0.034638],\t||\t recall@20\t[0.103564],\t||\t recall@50\t[0.184953]\n",
            "precision@5\t[0.260199],\t||\t precision@20\t[0.203750],\t||\t precision@50\t[0.151801]\n",
            "\n",
            "recall@5\t[0.035533],\t||\t recall@20\t[0.104266],\t||\t recall@50\t[0.186026]\n",
            "precision@5\t[0.263212],\t||\t precision@20\t[0.204884],\t||\t precision@50\t[0.152391]\n",
            "\n",
            "recall@5\t[0.035597],\t||\t recall@20\t[0.105273],\t||\t recall@50\t[0.187633]\n",
            "precision@5\t[0.264570],\t||\t precision@20\t[0.205861],\t||\t precision@50\t[0.153603]\n",
            "\n",
            "recall@5\t[0.035738],\t||\t recall@20\t[0.105629],\t||\t recall@50\t[0.188517]\n",
            "precision@5\t[0.264040],\t||\t precision@20\t[0.206606],\t||\t precision@50\t[0.153927]\n",
            "\n",
            "recall@5\t[0.036033],\t||\t recall@20\t[0.106001],\t||\t recall@50\t[0.189547]\n",
            "precision@5\t[0.266159],\t||\t precision@20\t[0.207392],\t||\t precision@50\t[0.154778]\n",
            "\n",
            "recall@5\t[0.035974],\t||\t recall@20\t[0.106421],\t||\t recall@50\t[0.190054]\n",
            "precision@5\t[0.265066],\t||\t precision@20\t[0.207732],\t||\t precision@50\t[0.154828]\n",
            "\n",
            "recall@5\t[0.036155],\t||\t recall@20\t[0.107098],\t||\t recall@50\t[0.190727]\n",
            "precision@5\t[0.266623],\t||\t precision@20\t[0.208841],\t||\t precision@50\t[0.155258]\n",
            "\n",
            "recall@5\t[0.036609],\t||\t recall@20\t[0.107566],\t||\t recall@50\t[0.190836]\n",
            "precision@5\t[0.267781],\t||\t precision@20\t[0.209247],\t||\t precision@50\t[0.155520]\n",
            "\n",
            "recall@5\t[0.036906],\t||\t recall@20\t[0.107670],\t||\t recall@50\t[0.191362]\n",
            "precision@5\t[0.267550],\t||\t precision@20\t[0.209114],\t||\t precision@50\t[0.155795]\n",
            "\n",
            "recall@5\t[0.036388],\t||\t recall@20\t[0.107743],\t||\t recall@50\t[0.191732]\n",
            "precision@5\t[0.267550],\t||\t precision@20\t[0.209073],\t||\t precision@50\t[0.156053]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cool_extension = BPRwith_item_popularity_user_item_bias(train_mat, test_mat, latent=5, lr=0.02, reg=0.001)\n",
        "cool_extension.train(epoch=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWKpnX8DfU7j"
      },
      "source": [
        "# Collaboration Declarations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg5RckUJfU7j"
      },
      "source": [
        "** You should fill out your collaboration declarations here.**\n",
        "\n",
        "**Reminder:** You are expected to complete each homework independently. Your solution should be written by you without the direct aid or help of anyone else. However, we believe that collaboration and team work are important for facilitating learning, so we encourage you to discuss problems and general problem approaches (but not actual solutions) with your classmates. You may post on Piazza, search StackOverflow, etc. But if you do get help in this way, you must inform us by filling out the Collaboration Declarations at the bottom of this notebook.\n",
        "\n",
        "Example: I found helpful code on stackoverflow at https://stackoverflow.com/questions/11764539/writing-fizzbuzz that helped me solve Problem 2."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7 (v3.10.7:6cc6b13308, Sep  5 2022, 14:02:52) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
